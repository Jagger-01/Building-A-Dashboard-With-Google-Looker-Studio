{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data Wrangling**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Identify duplicate values in the dataset.\n",
        "\n",
        "-   Remove duplicate values from the dataset.\n",
        "\n",
        "-   Identify missing values in the dataset.\n",
        "\n",
        "-   Impute the missing values in the dataset.\n",
        "\n",
        "-   Normalize data in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import pandas module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset into a dataframe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Read Data</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pyodide.http import pyfetch\n",
        "\n",
        "async def download(url, filename):\n",
        "    response = await pyfetch(url)\n",
        "    if response.status == 200:\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(await response.bytes())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To obtain the dataset, utilize the download() function as defined above:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "await download(file_path, \"m1_survey_data.csv\")\n",
        "file_name=\"m1_survey_data.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilize the Pandas method read_csv() to load the data into a dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding duplicates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Find how many duplicate rows exist in the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 154\n"
          ]
        }
      ],
      "source": [
        "num_duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing duplicates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove the duplicate rows from the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify if duplicates were actually dropped.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows after dropping: 0\n"
          ]
        }
      ],
      "source": [
        "num_duplicates_after = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows after dropping: {num_duplicates_after}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finding Missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find the missing values for all columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values for all columns:\n",
            "Respondent        0\n",
            "MainBranch        0\n",
            "Hobbyist          0\n",
            "OpenSourcer       0\n",
            "OpenSource       81\n",
            "               ... \n",
            "Sexuality       542\n",
            "Ethnicity       675\n",
            "Dependents      140\n",
            "SurveyLength     19\n",
            "SurveyEase       14\n",
            "Length: 85, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values for all columns:\")\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find out how many rows are missing in the column 'WorkLoc'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of missing rows in 'WorkLoc': 32\n"
          ]
        }
      ],
      "source": [
        "missing_workloc = df['WorkLoc'].isnull().sum()\n",
        "print(f\"Number of missing rows in 'WorkLoc': {missing_workloc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imputing missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find the  value counts for the column WorkLoc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value counts for 'WorkLoc':\n",
            "WorkLoc\n",
            "Office                                            6806\n",
            "Home                                              3589\n",
            "Other place, such as a coworking space or cafe     971\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "workloc_value_counts = df['WorkLoc'].value_counts()\n",
        "print(\"Value counts for 'WorkLoc':\")\n",
        "print(workloc_value_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identify the value that is most frequent (majority) in the WorkLoc column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most frequent value in 'WorkLoc': Office\n"
          ]
        }
      ],
      "source": [
        "majority_value = workloc_value_counts.idxmax()\n",
        "print(f\"Most frequent value in 'WorkLoc': {majority_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Impute (replace) all the empty rows in the column WorkLoc with the value that you have identified as majority.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df['WorkLoc'].fillna(majority_value, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After imputation there should ideally not be any empty rows in the WorkLoc column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify if imputing was successful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of missing rows in 'WorkLoc' after imputation: 0\n"
          ]
        }
      ],
      "source": [
        "missing_workloc_after = df['WorkLoc'].isnull().sum()\n",
        "print(f\"Number of missing rows in 'WorkLoc' after imputation: {missing_workloc_after}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are two columns in the dataset that talk about compensation.\n",
        "\n",
        "One is \"CompFreq\". This column shows how often a developer is paid (Yearly, Monthly, Weekly).\n",
        "\n",
        "The other is \"CompTotal\". This column talks about how much the developer is paid per Year, Month, or Week depending upon his/her \"CompFreq\". \n",
        "\n",
        "This makes it difficult to compare the total compensation of the developers.\n",
        "\n",
        "Create a new column called 'NormalizedAnnualCompensation' which contains the 'Annual Compensation' irrespective of the 'CompFreq'.\n",
        "\n",
        "Once this column is ready, it makes comparison of salaries easy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "List out the various categories in the column 'CompFreq'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categories in 'CompFreq':\n",
            "['Yearly' 'Monthly' 'Weekly' nan]\n"
          ]
        }
      ],
      "source": [
        "compfreq_categories = df['CompFreq'].unique()\n",
        "print(\"Categories in 'CompFreq':\")\n",
        "print(compfreq_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a new column named 'NormalizedAnnualCompensation'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  CompFreq  CompTotal  NormalizedAnnualCompensation\n",
            "0   Yearly    61000.0                       61000.0\n",
            "1   Yearly   138000.0                      138000.0\n",
            "2   Yearly    90000.0                       90000.0\n",
            "3  Monthly    29000.0                      348000.0\n",
            "4   Yearly    90000.0                       90000.0\n"
          ]
        }
      ],
      "source": [
        "def normalize_annual_compensation(row):\n",
        "    if row['CompFreq'] == 'Yearly':\n",
        "        return row['CompTotal']\n",
        "    elif row['CompFreq'] == 'Monthly':\n",
        "        return row['CompTotal'] * 12\n",
        "    elif row['CompFreq'] == 'Weekly':\n",
        "        return row['CompTotal'] * 52\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "df['NormalizedAnnualCompensation'] = df.apply(normalize_annual_compensation, axis=1)\n",
        "\n",
        "print(df[['CompFreq', 'CompTotal', 'NormalizedAnnualCompensation']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!--| Date (YYYY-MM-DD) | Version | Changed By        | Change Description                 |\n",
        "| ----------------- | ------- | ----------------- | ---------------------------------- |\n",
        "| 2020-10-17        | 0.1     | Ramesh Sannareddy | Created initial version of the lab |--!>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "90ac80ca41fde58b95f02cc0cfc0a0f2ed3115615ffe1bdfdfc10fd5ffeef9a4"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
